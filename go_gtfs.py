# -*- coding: utf-8 -*-
"""GO-GTFS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EaDyXTPVj0kMSAG9CJ7hX7-rC5VNaaV8

# **GO GTFS Data Analysis**

**Project Overview**

This notebook analyzes GTFS data from Metrolinx bus operations to extract insights related to trip volumes, stop activity, and trip durations. The analysis follows data validation and modular coding best practices.

#  1. Data Loading & Validation

### 1.1 Loading GTFS Data Files
"""

### 1.1 Loading GTFS Data Files

from google.colab import drive
drive.mount('/content/drive')

import os
import pandas as pd
import matplotlib.pyplot as plt

gtfs_path = "/content/drive/MyDrive/GO-GTFS"


files = os.listdir(gtfs_path)
files

gtfs_dfs = {}

for file in files:
    if file.endswith(".txt"):
        df_name = file.replace(".txt", "")
        gtfs_dfs[df_name] = pd.read_csv(
            f"{gtfs_path}/{file}",
            dtype=str
        )

gtfs_dfs.keys()

"""## 1.2 Data Quality Checks"""

#### Missing Values
#### Duplicate Records

data_quality_report = []


for name, df in gtfs_dfs.items():
    missing = df.isnull().sum().sum() #look for nulls in all cols and rows
    duplic = df.duplicated().sum() #look for duplicates in all rows
    rows = df.shape[0]
    cols = df.shape[1]

    data_quality_report.append({
        "Table": name,
        "Rows": rows,
        "Columns": cols,
        "Missing_Values": missing,
        "Duplicates": duplic

    })

null_details = []

for name, df in gtfs_dfs.items():
    for column in df.columns:
        null_count = df[column].isnull().sum()

        if null_count > 0:
            null_details.append({
                "File": name,
                "Column": column,
                "Null_Values": null_count
            })

"""## 1.3 Data Cleaning Actions"""

# Create a copy of the original dataframes to apply cleaning actions
clean_gtfs_dfs = {}

for name, df in gtfs_dfs.items():
    df_clean = df.copy()

    # 1. Remove duplicate rows
    initial_rows = len(df_clean)
    df_clean = df_clean.drop_duplicates()
    removed_duplicates = initial_rows - len(df_clean)

    # 2. Handle missing / null values
    # Strategy depends on the type of table and column importance

    if name == "stop_times":
        # Drop rows with missing critical time or stop information
        df_clean = df_clean.dropna(
            subset=["trip_id", "arrival_time", "departure_time", "stop_id"]
        )

    elif name == "trips":
        # Drop trips with missing route or service information
        df_clean = df_clean.dropna(
            subset=["route_id", "service_id", "trip_id"]
        )

    elif name == "routes":
        # Routes must have an id and short name
        df_clean = df_clean.dropna(
            subset=["route_id", "route_short_name"]
        )

    elif name == "stops":
        # Stops must have an id and coordinates
        df_clean = df_clean.dropna(
            subset=["stop_id", "stop_lat", "stop_lon"]
        )

    else:
        # For other tables, drop rows where all values are null
        df_clean = df_clean.dropna(how="all")

    # 3. Store cleaned dataframe
   # clean_gtfs_dfs[name] = df_clean

    # Optional: print summary of cleaning actions
   # print(f"{name}:")
   # print(f"  - Duplicates removed: {removed_duplicates}")
   # print(f"  - Rows after cleaning: {len(df_clean)}")
   # print("-" * 40)



"""# 2. Trip Analysis

## 2.1 Total Number of Trips per Route
"""

#trips per route for a given day

def trips_per_route(trips_df, date):
    filtered = trips_df[trips_df["service_id"] == date]
    return (
        filtered
        .groupby("route_id")
        .size()
        .reset_index(name="total_trips")
    )

"""## 2.2 Top 3 Routes with the Highest Number of Trips"""

#top 3 routes
def top_3_routes(trips_df):
     return (
         trips_df
        .groupby("route_id")
        .size()
        .sort_values (ascending=False)
        .head(3)
    )

"""# 3. Stop Frequency Analysis

## 3.1 Top 5 Stops During Peak Hours (7:00â€“9:00 AM)
"""

#Determine the top 5 bus stops with the highest number of arrivals during
#peak hours (e.g., 7:00 AM - 9:00 AM).

#Aux var for time (hour and minutes to seconds)
def time_to_seconds(t):
    h, m, s = map(int, t.split(":"))
    return h * 3600 + m * 60 + s


def top_stops_peak_hours(stop_times_df):
    stop_times_df["arrival_sec"] = stop_times_df["arrival_time"].apply(time_to_seconds)

    #defining peak hours between 7am and 9pm (seconds)
    peak = stop_times_df[
        (stop_times_df["arrival_sec"] >= 7 * 3600) &
        (stop_times_df["arrival_sec"] <= 9 * 3600)
    ]

    return (
        peak
        .groupby("stop_id")
        .size()
        .reset_index(name="arrivals")
        .sort_values("arrivals", ascending=False)
        .head(5)
    )

"""## 3.2 Distribution of Stop Arrivals Throughout the Day (Histogram)"""

#Plot a histogram showing the distribution of stop arrivals during the entire
#day.

def plot_arrival_distribution(stop_times_df):
    stop_times_df["arrival_sec"] = stop_times_df["arrival_time"].apply(time_to_seconds)
    plt.hist(stop_times_df["arrival_sec"], bins=24)
    plt.xlabel("Arrival time (seconds since midnight")
    plt.ylabel("Number of arrivals")
    plt.title("Daily arrival time distribution")
    plt.show()
    plt.close()

"""# 4. Trip Duration Estimation

## 4.1 Average Trip Duration per Route
"""

#Estimate the average duration of trips per route using stop_times.txt data

#Aux var for time (hour and minutes to seconds)
def time_to_seconds(t):
    h, m, s = map(int, t.split(":"))
    return h * 3600 + m * 60 + s

#Asumption: Consider only the arrival time (and not the departure time)
#to calculate the length of the trip

def trip_durations_with_route(stop_times_df, trips_df):
    # Trip lenght
    trip = stop_times_df.groupby("trip_id")["arrival_time"].agg(["min", "max"])
    trip["duration_sec"] = (
        trip["max"].apply(time_to_seconds) -
        trip["min"].apply(time_to_seconds)
    )

    trip = trip.reset_index()

    # Add route_id from trips.txt
    trip = trip.merge(
        trips_df[["trip_id", "route_id"]],
        on="trip_id",
        how="left"
    )

    #Calculate average by route
    avg_by_route = (
        trip
        .groupby("route_id")["duration_sec"]
        .mean()
        .reset_index()
    )

    avg_by_route["avg_duration_min"] = avg_by_route["duration_sec"] / 60

    return avg_by_route


import os
from datetime import datetime

def save_avg_by_route(avg_by_route_df, output_dir="outputs"):
    # Crate outputs if it is not exist
    os.makedirs(output_dir, exist_ok=True)

    # Timestamp
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"avg_by_route_{timestamp}.txt"
    filepath = os.path.join(output_dir, filename)

    # Save file
    avg_by_route_df.to_csv(
        filepath,
        sep="|",
        index=False
    )

    return filepath

"""## 4.2 Routes with unusually long or short trip durations"""

import os
import glob
import pandas as pd

def load_latest_avg_by_route(output_dir="outputs"):
    # Search all files avg_by_route
    files = glob.glob(os.path.join(output_dir, "avg_by_route_*.txt"))

    if not files:
        raise FileNotFoundError("No avg_by_route files found.")

    # Select the latest file
    latest_file = max(files, key=os.path.getmtime)

    # Load file
    df = pd.read_csv(latest_file, sep="|")

    return df, latest_file


avg_by_route, filename = load_latest_avg_by_route()

print(f"Lasted file saved: {filename}")
avg_by_route.head()

def identify_unusual_routes(avg_by_route_df):
    p10 = avg_by_route_df["duration_sec"].quantile(0.10)
    p90 = avg_by_route_df["duration_sec"].quantile(0.90)

    unusually_short = (
        avg_by_route_df[
            avg_by_route_df["duration_sec"] <= p10
        ]
        .sort_values(by="duration_sec", ascending=True)
        .reset_index(drop=True)
    )

    unusually_long = (
        avg_by_route_df[
            avg_by_route_df["duration_sec"] >= p90
        ]
        .sort_values(by="duration_sec", ascending=False)
        .reset_index(drop=True)
    )

    return unusually_short, unusually_long

"""# 5. Execution

"""

#Quality Summary
quality_df = pd.DataFrame(data_quality_report)
quality_df

#null details

null_columns_df = pd.DataFrame(null_details)
null_columns_df

#Trips per route for a given day
date = "20260402"
trips_df = gtfs_dfs["trips"]
trips_per_route (trips_df,date)

#top 3 routes with the highest number of trips
trips_df = gtfs_dfs["trips"]
top_3_routes(trips_df)

# 5 top stops peak hours
stop_times = gtfs_dfs["stop_times"]
top_stops_peak_hours (stop_times)

# Histogram distribution of stop arrivals during the entire day, using time in seconds
stop_times = gtfs_dfs["stop_times"]
plot_arrival_distribution(stop_times)

#average trip duration
stop_times_df = gtfs_dfs["stop_times"]
trips_df = gtfs_dfs["trips"]

avg_by_route = trip_durations_with_route(stop_times_df, trips_df)
file_name = save_avg_by_route(avg_by_route)

print(f"File saved as: {file_name}")

#Highlight routes with unusually long or short trip durations.

avg_by_route, filename = load_latest_avg_by_route()

print(f"Lasted file saved: {filename}")
avg_by_route.head()

short_routes, long_routes = identify_unusual_routes(avg_by_route)

print("Unusually short routes:")
print(short_routes)

print("\nUnusually long routes:")
print(long_routes)